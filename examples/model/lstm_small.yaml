# lstm.yaml

model:
  class_path: lib.training.modules.PipelineModule
  init_args:
    model:
      class_path: lib.models.recurrent.RecurrentEncoderDecoderWithAttention
      init_args:
        embedding_dim: 32
        hidden_size: 32
        out_size: 2
        num_encoder_layers: 6
        num_decoder_layers: 6
        dropout: 0.2
        num_embeddings: 4
        num_heads: 8
        attention_dropout: 0.0
        pooling:
          type: mean
          dim: -2
    objectives:
      loss:
        class_path: torch.nn.MSELoss
    name: lstm

data:
  batch_size: 64
  input_variables:
    - sequence_embedding